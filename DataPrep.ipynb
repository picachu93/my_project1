{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "# CrisisLex\n",
    "# Author: Alexandra Olteanu\n",
    "# Check LICENSE for details about copyright.\n",
    "\n",
    "import json\n",
    "\n",
    "#receives a string in json format\n",
    "#returns the textual content of a tweet\n",
    "def extract_tweet_from_json(data):\n",
    "    try:\n",
    "        json_tweet = json.loads(data.strip())\n",
    "    except:\n",
    "        exit(\"Not able to load json data\")\n",
    "    if 'text' in json_tweet:\n",
    "        return json_tweet['text'].replace('\\n','')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# reads the terms to be tracked from a file\n",
    "# expects one term per line\n",
    "def get_query_terms(input_filename):\n",
    "    query_terms = []\n",
    "    for line in input_filename:\n",
    "        query_terms.append(line.strip())\n",
    "    return query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load config.py\n",
    "#Sets Twitter API access\n",
    "\n",
    "#Please give the keys as 'strings'\n",
    "CONSUMER_KEY = 'm4pfoQYtQOCahpKEY55dlogg7' # API key\n",
    "CONSUMER_SECRET = 'TdU3rKhwfspQYrSA0GsPW7IooRkbw9opfZ82KZmADsGvnAUL01' # API secret\n",
    "ACCESS_KEY = '798171014693879808-9I6Ms2tOWxLmXlwVbNHmEoYQJxO97rB' # Access token\n",
    "ACCESS_SECRET= '3vEM0E6i6OfmNE95ng8VcK6dHtKS6qByQoD7mZtp7qqvb' # Access token secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring query settings ....\n",
      "Collecting tweets ....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-894b7e5df38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Collecting tweets ....\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_track\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlocations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6.38\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m49.87\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.77\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m55.81\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"The script have crashed with the following error: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/my_project1/tweepy1/streaming.pyc\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, follow, track, async, locations, count, stall_warnings, languages)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlencode_noplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delimited'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'length'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdisconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/my_project1/tweepy1/streaming.pyc\u001b[0m in \u001b[0;36m_start\u001b[0;34m(self, async)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/my_project1/tweepy1/streaming.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m                     \u001b[0merror_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/my_project1/tweepy1/streaming.pyc\u001b[0m in \u001b[0;36m_read_loop\u001b[0;34m(self, resp)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\n'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misclosed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mdelimited_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda2/lib/python2.7/httplib.pyc\u001b[0m in \u001b[0;36m_read_chunked\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunk size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda2/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda2/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                     self.__class__)\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/anaconda2/lib/python2.7/ssl.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import tweepy1 as t\n",
    "from tweepy1.parsers import ModelParser\n",
    "from tweepy1 import StreamListener\n",
    "from tweepy1 import Stream\n",
    "\n",
    "\n",
    "import utils1\n",
    "#import config as c\n",
    "\n",
    "class PrintListener(StreamListener):\n",
    "    output = None\n",
    "    def on_data(self, data):\n",
    "        if self.output is None:\n",
    "            print data\n",
    "            return True\n",
    "        else:\n",
    "            print>>self.output, data.strip()\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            print status, \"Twitter API Error: Enhance your calm -- You are being rate limited\"\n",
    "        elif status == 401:\n",
    "            print status, \"Twitter API Error: Unauthorized -- Authentication credentials were missing or incorrect. Please double check config.py\"\n",
    "        else:\n",
    "            print status\n",
    "        \n",
    "    def set_output(self, output_json):\n",
    "        self.output = output_json\n",
    "\n",
    "#authenticate\n",
    "auth = t.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "api = t.API(auth_handler=auth, parser = ModelParser())      \n",
    "        \n",
    "print \"Configuring query settings ....\"  \n",
    "    \n",
    "filename = 'data/test3.txt'\n",
    "lexicon = 'QuakeLex.txt'\n",
    "pl = PrintListener()\n",
    "pl.set_output(open(filename,\"w\"))\n",
    "\n",
    "try:\n",
    "        to_track = utils1.get_query_terms(open(lexicon,\"r\"))\n",
    "except Exception as e:\n",
    "        print \"The file path is seems to be wrong. Check the error below or run the script with -h. Please revise and restart the script\"\n",
    "        print e\n",
    "        exit(0)\n",
    "        \n",
    "#start tracking crisis-relevant tweets\n",
    "stream = Stream(auth, pl)\n",
    "try:\n",
    "    print \"Collecting tweets ....\"\n",
    "    #stream.filter(track=to_track[0:400],locations=[-6.38,49.87,1.77,55.81],languages=[\"en\"])\n",
    "    stream.filter(track=to_track[0:400],languages=[\"en\"])\n",
    "except Exception as e:\n",
    "    print \"The script have crashed with the following error: \"\n",
    "    print e\n",
    "    print \"\\n Please check if your Twitter API keys are correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load adaptive_collect.py\n",
    "# CrisisLex\n",
    "# Author: Alexandra Olteanu\n",
    "# Check LICENSE for details about copyright.\n",
    "\n",
    "import sys\n",
    "import utils\n",
    "import os\n",
    "\n",
    "#handling time\n",
    "import datetime\n",
    "\n",
    "#tweepy1\n",
    "import tweepy1 as t\n",
    "from tweepy1.parsers import ModelParser\n",
    "from tweepy1 import StreamListener\n",
    "from tweepy1 import Stream\n",
    "\n",
    "\n",
    "# authentication parameters\n",
    "import config as c\n",
    "\n",
    "#nlp processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#processes the tweet and updates hashtag_fd\n",
    "#specifically, if the hashtag was already encountered it adds it to the freq dict,\n",
    "# otherwise it increases the hashtag counter\n",
    "def update_hashtags_stats(hashtags_fd, json_tweet):\n",
    "    tweet = utils.extract_tweet_from_json(json_tweet)\n",
    "    tweet_terms = []\n",
    "    if tweet is None or '#' not in tweet:\n",
    "        return False\n",
    "    tokenizer = nltk.RegexpTokenizer('\\#?[\\w\\d]+')\n",
    "    doc = tokenizer.tokenize(tweet)\n",
    "    for w_raw in doc:\n",
    "        if '#' not in w_raw:\n",
    "            continue\n",
    "        w = (w_raw.strip('\\\"\\'.,;?!:)(@/*&')).lower()\n",
    "        tweet_terms.append(w)\n",
    "        hashtags_fd.inc(w)\n",
    "    return True\n",
    "\n",
    "\n",
    "#processes the tweet and updates terms_fd based on the tweet terms\n",
    "#specifically, if the term was already encountered it adds it to the freq dict,\n",
    "# otherwise it increases the term counter\n",
    "def update_terms_stats(terms_fd, json_tweet, lex):\n",
    "    tweet = utils.extract_tweet_from_json(json_tweet)\n",
    "    tweet_terms = []\n",
    "    if tweet is None:\n",
    "        return False\n",
    "    tokenizer = nltk.RegexpTokenizer('\\#?[\\w\\d]+')\n",
    "    doc = tokenizer.tokenize(tweet)\n",
    "    for w_raw in doc:\n",
    "        w = w_raw.strip('\\\"\\'.,;?!:)(@/*&')\n",
    "        if not (w.strip('#')).isalpha():\n",
    "            w_aux = ''\n",
    "            #ignore non-ascii characters\n",
    "            for s in w:\n",
    "                if ord(s) < 128:\n",
    "                    w_aux += s\n",
    "                else:\n",
    "                    break\n",
    "            w = w_aux\n",
    "        w = w.lower()\n",
    "        if (w not in stopwords.words('english') and w not in set(['rt','http','amp'])) and len(w) in range(3, 16):\n",
    "            if w in lex:\n",
    "                continue\n",
    "            tweet_terms.append(w)\n",
    "            terms_fd.inc(w)\n",
    "    bigrams = nltk.bigrams(tweet_terms)\n",
    "    for b in bigrams:\n",
    "        if b[1]+\" \"+b[0] in lex or b[0]+\" \"+b[1] in lex:\n",
    "            continue\n",
    "        if b[1]+\" \"+b[0] in terms_fd:\n",
    "            terms_fd.inc(b[1]+\" \"+b[0])\n",
    "        else:\n",
    "            terms_fd.inc(b[0]+\" \"+b[1])\n",
    "    return True\n",
    "\n",
    "\n",
    "class AdaptiveListener(StreamListener):\n",
    "    output = None\n",
    "    adaptive = False\n",
    "    start_time = None\n",
    "    end_time = None\n",
    "    terms = None\n",
    "    terms_fd = None\n",
    "    terms_no = 0\n",
    "    lex_set = None\n",
    "    use_hashtags = True\n",
    "\n",
    "    def on_data(self, data):\n",
    "        #prints to screen the filtered tweets\n",
    "        if self.output is None:\n",
    "            print data\n",
    "            return True\n",
    "\n",
    "        #prints to file the filtered tweets without any other actions\n",
    "        print>>self.output, data.strip()\n",
    "        if self.adaptive is False:\n",
    "            return True\n",
    "\n",
    "        #collects statistics from the tweets collected in the first hours and prints the tweets to file\n",
    "        if self.adaptive:\n",
    "            if datetime.datetime.now()>self.end_time:\n",
    "                self.terms = self.terms_fd.keys()[:self.terms_no]\n",
    "                print \"Adding to the query the following terms:\"\n",
    "                print self.terms\n",
    "                return False\n",
    "\n",
    "            if self.use_hashtags:\n",
    "                update_hashtags_stats(self.terms_fd,data)\n",
    "            else:\n",
    "                update_terms_stats(self.terms_fd,data,self.lex_set)\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            print status, \"Twitter API Error: Enhance your calm -- You are being rate limited\"\n",
    "        elif status == 401:\n",
    "            print status, \"Twitter API Error: Unauthorized -- Authentication credentials were missing or incorrect. Please double check config.py\"\n",
    "        else:\n",
    "            print status\n",
    "\n",
    "    def set_output(self, output_json):\n",
    "        self.output = output_json\n",
    "\n",
    "    def set_adaptive(self, lex, learning_time=3, use_hashtags=True, new_terms_no=10):\n",
    "        self.adaptive = True\n",
    "        self.lex_set = set(lex)\n",
    "        self.terms_no = new_terms_no\n",
    "        self.use_hashtags = use_hashtags\n",
    "\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.end_time = self.start_time + datetime.timedelta(hours=learning_time)\n",
    "        print \"Learning interval between %s to %s\"%(self.start_time,self.end_time)\n",
    "\n",
    "        self.terms_fd = nltk.FreqDist()\n",
    "        self.terms = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    filename = 'data/test3.txt'\n",
    "    lexicon = 'CrisisLexRec.txt'\n",
    "    prf_time = 1\n",
    "    adaptive = False\n",
    "    hashtags = 1\n",
    "    new_terms_no = 5\n",
    "\n",
    "    #authenticate\n",
    "    auth = t.OAuthHandler(c.CONSUMER_KEY, c.CONSUMER_SECRET)\n",
    "    auth.set_access_token(c.ACCESS_KEY, c.ACCESS_SECRET)\n",
    "    api = t.API(auth_handler=auth, parser=ModelParser())\n",
    "\n",
    "    #set up the stream listener\n",
    "    pl = AdaptiveListener()\n",
    "    pl.set_output(open(filename,\"w\"))\n",
    "\n",
    "    # get lexicon terms\n",
    "    try:\n",
    "        to_track = utils.get_query_terms(open(lexicon,\"r\"))\n",
    "    except Exception as e:\n",
    "        print \"The file path is seems to be wrong. Check the error below or run the script with -h. Please revise and restart the script\"\n",
    "        print e\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "    #set the learning time\n",
    "    if adaptive:\n",
    "        pl.set_adaptive(to_track, prf_time, (hashtags == 1), new_terms_no)\n",
    "\n",
    "    #start tracking crisis-relevant tweets\n",
    "    stream = Stream(auth, pl)\n",
    "    stream.filter(track=to_track[0:400],locations=[-6.38,49.87,1.77,55.81],languages=[\"en\"])\n",
    "\n",
    "    print \"New query...\"\n",
    "    #add new terms to be tracked and restart the crawling.\n",
    "    if pl.adaptive:\n",
    "        assert pl.terms\n",
    "        to_track[0:0] = pl.terms\n",
    "\n",
    "    pl.adaptive = False\n",
    "    stream.filter(track=to_track[0:400],locations=[-6.38,49.87,1.77,55.81],languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweets_data_path = './data/test3.txt'\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print len(tweets_data) \n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = map(lambda tweet: tweet.get('text', None),tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet.get('lang', None),tweets_data)\n",
    "tweets['country'] = map(lambda tweet: tweet.get('place').get('country', None) if tweet.get('place') != None else None, tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Languages', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar', color='red')\n",
    "\n",
    "tweets_by_country = tweets['country'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Countries', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 countries', fontsize=15, fontweight='bold')\n",
    "tweets_by_country[:5].plot(ax=ax, kind='bar', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('2012_Sandy_Hurricane-ontopic_offtopic.csv','rU') as fin, open ('outfile2','w') as fout:\n",
    "    writer = csv.writer(fout, delimiter=',')\n",
    "    for row in csv.reader(fin, delimiter=','):\n",
    "        if row[2] == 'on-topic':\n",
    "             writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"nepal_neg_tott.csv\", \"rb\") as source:\n",
    "    lines = [line for line in source]\n",
    "import random\n",
    "random_choice = random.sample(lines, 4500)\n",
    "with open(\"nepal_neg1_tot.csv\", \"wb\") as sink:\n",
    "    sink.write(\"\".join(random_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('nepal_pos.json') as tweet_data:\n",
    "    json_data = json.load(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "\n",
    "tweets_data_path = 'nepal_neg.json'\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "fout = open('nepal_neg_txt.csv','w+')\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        date = tweet['created_at']\n",
    "        id = tweet['id']\n",
    "        if 'retweeted_status' in tweet:\n",
    "            text = tweet['retweeted_status']['text']\n",
    "        else:\n",
    "            text = tweet['text']\n",
    "        #print(text)    \n",
    "        try:\n",
    "            c = time.strptime(date.replace(\"+0000\",''), '%a %b %d %H:%M:%S %Y')\n",
    "        except: \n",
    "            print \"pb with tweet_gmttime\", tweet_gmttime, line\n",
    "            pass\n",
    "        tweet_unixtime = int(time.mktime(c))\n",
    "        #fout.write(str(text) + \"\\t\" + str(id) + \"\\n\")\n",
    "        fout.write(str([text]) + \"\\n\")\n",
    "        #print(tweet_unixtime)\n",
    "        #tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "tweets_file.close()        \n",
    "fout.close()         \n",
    "#tweets = pd.DataFrame()\n",
    "#tweets['text'] = map(lambda tweet: tweet.get('text', None),tweets_data)\n",
    "#tweets_by_text = tweets['text'].value_counts()\n",
    "#f1=open('output5.txt', 'w+')\n",
    "#print >> f1,tweets_by_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
