{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml\n",
      "Checking https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml.sha1\n",
      "Checked https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml.sha1\n",
      "Checked https://repo1.maven.org/maven2/joda-time/joda-time/maven-metadata.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.6`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.1.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.apache.hadoop:hadoop-aws:2.6.4`\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                    \u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-mllib:2.1.0`\n",
    "import $ivy.`org.apache.bahir:spark-streaming-twitter_2.11:2.1.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.streaming.twitter._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.twitter._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.streaming.dstream.DStream\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.streaming.{Seconds, StreamingContext}\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@4d85c7fa"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = JupyterSparkSession.builder() // important - call this rather than SparkSession.builder()\n",
    "  .jupyter() // this method must be called straightaway after builder()\n",
    "  // .yarn(\"/etc/hadoop/conf\") // optional, for Spark on YARN - argument is the Hadoop conf directory\n",
    "  // .emr(\"2.6.4\") // on AWS ElasticMapReduce, this adds aws-related to the spark jar list\n",
    "   .master(\"local\") // change to \"yarn-client\" on YARN\n",
    "  // .config(\"spark.executor.instances\", \"10\")\n",
    "  // .config(\"spark.executor.memory\", \"3g\")\n",
    "  // .config(\"spark.hadoop.fs.s3a.access.key\", awsCredentials._1)\n",
    "  // .config(\"spark.hadoop.fs.s3a.secret.key\", awsCredentials._2)\n",
    "  .appName(\"notebook\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkConfiguration\u001b[39m: \u001b[32mSparkConf\u001b[39m = org.apache.spark.SparkConf@180263a2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkConfiguration = new SparkConf().\n",
    "    setAppName(\"spark-twitter-stream-example\").\n",
    "    setMaster(sys.env.get(\"spark.master\").getOrElse(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:",
      "org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)",
      "jupyter.spark.session.JupyterSparkSession$Builder.getOrCreate(JupyterSparkSession.scala:48)",
      "$sess.cmd4Wrapper$Helper.<init>(cmd4.sc:11)",
      "$sess.cmd4Wrapper.<init>(cmd4.sc:209)",
      "$sess.cmd4$.<init>(cmd4.sc:194)",
      "$sess.cmd4$.<clinit>(cmd4.sc)",
      "$sess.cmd4.$main(cmd4.sc)",
      "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
      "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
      "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
      "java.lang.reflect.Method.invoke(Method.java:498)",
      "ammonite.runtime.EvaluatorImpl.evalMain(Evaluator.scala:112)",
      "ammonite.runtime.EvaluatorImpl$$anonfun$processLine$2$$anonfun$apply$5$$anonfun$4.apply(Evaluator.scala:150)",
      "ammonite.runtime.Evaluator$.evaluatorRunPrinter(Evaluator.scala:233)",
      "ammonite.runtime.EvaluatorImpl$$anonfun$processLine$2$$anonfun$apply$5.apply(Evaluator.scala:150)",
      "ammonite.runtime.EvaluatorImpl$$anonfun$processLine$2$$anonfun$apply$5.apply(Evaluator.scala:146)",
      "ammonite.util.Catching.map(Res.scala:111)",
      "ammonite.runtime.EvaluatorImpl$$anonfun$processLine$2.apply(Evaluator.scala:146)",
      "ammonite.runtime.EvaluatorImpl$$anonfun$processLine$2.apply(Evaluator.scala:144)",
      "ammonite.util.Res$Success.flatMap(Res.scala:58)\u001b[39m",
      "  org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2278\u001b[39m)",
      "  org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2274\u001b[39m)",
      "  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m257\u001b[39m)",
      "  org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2274\u001b[39m)",
      "  org.apache.spark.SparkContext$.markPartiallyConstructed(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2353\u001b[39m)",
      "  org.apache.spark.SparkContext.<init>(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m85\u001b[39m)",
      "  org.apache.spark.streaming.StreamingContext$.createNewSparkContext(\u001b[32mStreamingContext.scala\u001b[39m:\u001b[32m837\u001b[39m)",
      "  org.apache.spark.streaming.StreamingContext.<init>(\u001b[32mStreamingContext.scala\u001b[39m:\u001b[32m84\u001b[39m)",
      "  $sess.cmd7Wrapper$Helper.<init>(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m1\u001b[39m)",
      "  $sess.cmd7Wrapper.<init>(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m231\u001b[39m)",
      "  $sess.cmd7$.<init>(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m205\u001b[39m)",
      "  $sess.cmd7$.<clinit>(\u001b[32mcmd7.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "\n",
    "val streamingContext = new StreamingContext(sparkConfiguration, Seconds(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7a362253"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd5.sc:2: value parallelize is not a member of org.apache.spark.sql.SparkSession\n",
      "val distData = sparkSession.parallelize(data)\n",
      "                            ^"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "val data = Array(1, 2, 3, 4, 5)\n",
    "val distData = sparkSession.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msparkSession.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparkSession.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sparkSession.read.text(\"nepal_pos_list.txt\").as[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mwords\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mString\u001b[39m] = [value: string]\n",
       "\u001b[36mgroupedWords\u001b[39m: \u001b[32mKeyValueGroupedDataset\u001b[39m[\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m] = org.apache.spark.sql.KeyValueGroupedDataset@3fba02d9\n",
       "\u001b[36mcounts\u001b[39m: \u001b[32mDataset\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mLong\u001b[39m)] = [value: string, count(1): bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words = data.flatMap(value => value.split(\"\\\\s+\"))\n",
    "val groupedWords = words.groupByKey(_.toLowerCase)\n",
    "val counts = groupedWords.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               value|count(1)|\n",
      "+--------------------+--------+\n",
      "|               those|      81|\n",
      "|           kathmandu|     358|\n",
      "|                hope|      56|\n",
      "|                some|      66|\n",
      "|              mahaan|      11|\n",
      "|            medicare|       4|\n",
      "|             report\"|       3|\n",
      "|             \\n\\nyet|       2|\n",
      "|               still|      37|\n",
      "|             buried\"|       1|\n",
      "|situation\\n#earth...|       2|\n",
      "|         chetrapathi|       1|\n",
      "|                 few|       9|\n",
      "|              online|       5|\n",
      "|devastating\\ncala...|       1|\n",
      "|               \"with|       2|\n",
      "|         \"absolutely|       4|\n",
      "|        \"transparent|       1|\n",
      "|                 art|       1|\n",
      "|                 gir|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
