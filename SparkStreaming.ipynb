{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Spark\n",
    "from pyspark import SparkContext\n",
    "#    Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "#    Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "#    json parsing\n",
    "import json\n",
    "#    print function\n",
    "from __future__ import print_function\n",
    "#    os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zkQuorum = \"localhost:2181\"\n",
    "topic = \"twitter-stream\"\n",
    "seconds_to_run = 60\n",
    "ssc = StreamingContext(sc, seconds_to_run)\n",
    "\n",
    "tweets = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the inbound message as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inbound stream is a DStream, which supports various built-in transformations such as map which is used here to parse the inbound messages from their native JSON format.\n",
    "Note that this will fail horribly if the inbound message isn't valid JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = tweets.map(lambda v: json.loads(v[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed.count().map(lambda x:'Tweets in this batch: %s' % x).pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dstream = parsed.map(lambda tweet: tweet['text'])\n",
    "#text_dstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the extracted text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_out_unicode(tweet):\n",
    "  \n",
    "    try:\n",
    "        clean_tweet = str(tweet)\n",
    "    except UnicodeEncodeError:\n",
    "        pass\n",
    "    return clean_tweet\n",
    "\n",
    "def expand_around_chars(text, characters):\n",
    "    for char in characters:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    return text\n",
    "\n",
    "def strip_quotations_newline(text):\n",
    "    clean_tweet = ' '.join(text.split())\n",
    "    clean_tweet = clean_tweet.encode('utf-8')\n",
    "    clean_tweet = clean_tweet.replace('\",\\'','')\n",
    "    clean_tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'''(@[A-Za-z0-9]+)''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"([0-9]+)\", \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'[^\\x00-\\x7F]+','', clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "def split_text(text):\n",
    "    text = strip_quotations_newline(text)\n",
    "    text = expand_around_chars(text, '\\/\".,()&[]{}:;!-_\\'')\n",
    "    splitted_text = text.split(' ')\n",
    "    cleaned_text = [x for x in splitted_text if len(x) > 2]\n",
    "    text_lowercase = [x.lower() for x in cleaned_text]\n",
    "    return text_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mess = text_dstream.map(lambda text: split_text(text))\n",
    "mess.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create clean tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to be classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(numFeatures=50000)\n",
    "fh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_tweet(tf):\n",
    "    idf = IDF().fit(tf)\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:49:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 97\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:49:00\n",
      "-------------------------------------------\n",
      "stay the fuck outta their hood then\n",
      "any teenation member beat quake champions will recieve till reach you lose will\n",
      "panther whales out here hittin licks seen all\n",
      "ara won shaking #whatsmynamestwin\n",
      "global unrest event underway west coast usa canada hit earthquake\n",
      "jonny amp thom play one off show macerata sferisterio august proceeds will help the restoration earthq\n",
      "after you predicted crooked hilly landslide you now are saying maybe and maybe not? lol\n",
      "blackpink shaking #krunk\n",
      "scoffs shaking her head please outside often both know that\n",
      "jay panorama reported people lived that tower survivors are really suggesting only are dead\n",
      "...\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33.awaitTerminationOrTimeout.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/streaming/dstream.py\", line 159, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"<ipython-input-10-977c4d20cde0>\", line 1, in <lambda>\n    tweet_df = mess.foreachRDD(lambda rdd: rdd.toDF(\"text\"))\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/session.py\", line 57, in toDF\n    return sparkSession.createDataFrame(self, schema, sampleRatio)\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/session.py\", line 492, in createDataFrame\n    schema = _parse_datatype_string(schema)\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/types.py\", line 845, in _parse_datatype_string\n    return _parse_basic_datatype_string(s)\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/types.py\", line 739, in _parse_basic_datatype_string\n    raise ValueError(\"Could not parse datatype: %s\" % s)\nValueError: Could not parse datatype: text\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:247)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:246)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-562afce8b755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/streaming/context.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mawaitTerminationOrTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.awaitTerminationOrTimeout.\n: org.apache.spark.SparkException: An exception was raised by Python:\nTraceback (most recent call last):\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/streaming/util.py\", line 65, in call\n    r = self.func(t, *rdds)\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/streaming/dstream.py\", line 159, in <lambda>\n    func = lambda t, rdd: old_func(rdd)\n  File \"<ipython-input-10-977c4d20cde0>\", line 1, in <lambda>\n    tweet_df = mess.foreachRDD(lambda rdd: rdd.toDF(\"text\"))\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/session.py\", line 57, in toDF\n    return sparkSession.createDataFrame(self, schema, sampleRatio)\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/session.py\", line 492, in createDataFrame\n    schema = _parse_datatype_string(schema)\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/types.py\", line 845, in _parse_datatype_string\n    return _parse_basic_datatype_string(s)\n  File \"/usr/local/src/spark/spark-2.0.2-bin-hadoop2.7/python/pyspark/sql/types.py\", line 739, in _parse_basic_datatype_string\n    raise ValueError(\"Could not parse datatype: %s\" % s)\nValueError: Could not parse datatype: text\n\n\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.api.python.PythonDStream$$anonfun$callForeachRDD$1.apply(PythonDStream.scala:179)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)\n\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:247)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:247)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:246)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:50:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 61\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:50:00\n",
      "-------------------------------------------\n",
      "the whales are fed with our shit\n",
      "she shaking them titties girl yes\n",
      "gangs aggressive killer whales are shaking down alaska fishing boats for their fish report\n",
      "imagine this dance king hit the stage already shaking\n",
      "aggressive shaking down\n",
      "your area expertise sir\n",
      "got shaking hands during #mmvas slow hands\n",
      "ara won shaking #whatsmynamestwin\n",
      "your polls have never been close being right they were hillary would won landslide fire your polling agent lol\n",
      "residents flee after tsunami hit greenland following earthquake\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:51:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 41\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:51:00\n",
      "-------------------------------------------\n",
      "imagine this dance king hit the stage already shaking\n",
      "omg after years finally got meet fucking shaking picture fine least looking\n",
      "qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "gangs aggressive killer whales are shaking down alaska fishing boats for their fish report\n",
      "earthquake banda sea\n",
      "dek qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "the fake left wants overthrow president who won landslide not counting illegal votes install globalist\n",
      "wth quakes rogue tsunami #yellowstone supervolcano all waking hell breaking loose? videos #paranormal\n",
      "gotta hear both sides yooo can see the dolphin them this pic\n",
      "helping residents affected #grenfell confidential free legal advice please get touch can help https\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:52:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 61\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:52:00\n",
      "-------------------------------------------\n",
      "bye packing things leaving earth bcs this planet will gonna chaos noisy shaking bcs these two\n",
      "cnn tries conduct online trump hater poll but turns out landslide support for president\n",
      "the fake left wants overthrow president who won landslide not counting illegal votes install globalist\n",
      "#ge lost opportunity improve public understanding brexit according latest commentary from\n",
      "stay the fuck outta their hood then\n",
      "jonny amp thom play one off show macerata sferisterio august proceeds will help the restoration earthq\n",
      "beginning look lot like unmasking throughout whitehouse corrruption disruption amp bribery amp never\n",
      "jay panorama reported people lived #grenfelltower there are reported survivors yet authorities say only are missi\n",
      "panther whales out here hittin licks seen all\n",
      "panther whales out here hittin licks seen all\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:53:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 43\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:53:00\n",
      "-------------------------------------------\n",
      "qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "the economist europes saviour? via\n",
      "air #nowplaying landslide original radio ashford\n",
      "jay panorama reported people lived #grenfelltower there are reported survivors yet authorities say only are missi\n",
      "qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "solar alert quake watch peaks yellowstone news june via\n",
      "panther whales out here hittin licks seen all\n",
      "nigga they are called killer whales not cuddly whales not mild mannered whales killer killer motherfuck\n",
      "panther whales out here hittin licks seen all\n",
      "welllllll the fishing boats are taking food from the killer whales tell the fishermen gtfo before them whales\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:54:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 64\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:54:00\n",
      "-------------------------------------------\n",
      "panther whales out here hittin licks seen all\n",
      "imagine group killer whales pullin\n",
      "blackpink teaser already has views antis are shaking\n",
      "jonny amp thom play one off show macerata sferisterio august proceeds will help the restoration earthq\n",
      "panther whales out here hittin licks seen all\n",
      "nigga they are called killer whales not cuddly whales not mild mannered whales killer killer motherfuck\n",
      "nigga they are called killer whales not cuddly whales not mild mannered whales killer killer motherfuck\n",
      "the usa already secretly getting bombed for the sri lanka tsunami and haiti genocides? erdogan\n",
      "solar alert quake watch peaks yellowstone news june\n",
      "oooo this bitch shaking the table\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:55:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 56\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:55:00\n",
      "-------------------------------------------\n",
      "whitechapel the east london mosque our building the month\n",
      "qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "earthquake occurred near offshore valparaiso chile utc #earthquake #offshorevalparaiso\n",
      "can handle this like gentleman can get into some gangsta shit\n",
      "wings short films gonna nothing compared what the budget and production value for this going honestly sha\n",
      "beth cnn tries conduct online trump hater poll but turns out landslide support for president\n",
      "hazard why #earthquake sequences happen? simple numerical simulation risk #jupyter #fa\n",
      "imagine getting rolled gang killer wales for fish\n",
      "\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:56:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 53\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:56:00\n",
      "-------------------------------------------\n",
      "the fake left wants overthrow president who won landslide not counting illegal votes install globalist\n",
      "omg after years finally got meet fucking shaking picture fine least looking\n",
      "this scared panda held onto police officer after earthquake\n",
      "civilians reported dead after fudged led strike syria reaction far\n",
      "ara won shaking #whatsmynamestwin\n",
      "qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "gangs aggressive killer whales are shaking down alaska fishing boats for their fish report\n",
      "ara won shaking #whatsmynamestwin\n",
      "your health being affected fragrances? have fantastic psychosensory technique help you check this\n",
      "stay the fuck outta their hood then\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:57:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 42\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:57:00\n",
      "-------------------------------------------\n",
      "fucking shaking love her much\n",
      "why are hands shaking not even nervous\n",
      "mag quake mon june located miles rose hill kansas details from\n",
      "swimming pool during earthquake\n",
      "monsters\n",
      "yes this damn cute girly\n",
      "models are shaking #choicesongmaleartist #teenchoice\n",
      "qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "secure the bag\n",
      "day damn what genius namjoon shaking\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:58:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 53\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:58:00\n",
      "-------------------------------------------\n",
      "mes shaking never ignore bts word never know when where they dropping hints\n",
      "jonny amp thom play one off show macerata sferisterio august proceeds will help the restoration earthq\n",
      "she shaking them titties girl yes\n",
      "ara won shaking #whatsmynamestwin\n",
      "imagine this dance king hit the stage already shaking\n",
      "shaking\n",
      "died because failed perdict earthquake\n",
      "civilians reported dead after fudged led strike syria reaction far\n",
      "blackpink shaking #krunk\n",
      "panther whales out here hittin licks seen all\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:59:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 50\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 14:59:00\n",
      "-------------------------------------------\n",
      "panther whales out here hittin licks seen all\n",
      "ing doyoung noticed yuta hands shaking out nervousness and held his hands calm his nerves down\n",
      "indian army rescued people stranded due landslide near bhalukpong arunachal pradesh\n",
      "yellowstone supervolcano hit swarm more than earthquakes one week #news #biblepro\n",
      "#indianarmy rescues lives stranded due landslide near bhalukpong arunachal pradesh #foreverforyou\n",
      "imagine this dance king hit the stage already shaking\n",
      "magnitude earthquake has occurred near anza united states\n",
      "mag quake mon june located miles rose hill kansas details from\n",
      "maguire may deciding bad deal better than deal with the dup she shaking the magic money tree buy votes many cli\n",
      "shaking\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 15:00:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 52\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 15:00:00\n",
      "-------------------------------------------\n",
      "sounds like something jamaican killer whales would affi adapt when times get rough pon utes dem nyam next\n",
      "wings short films gonna nothing compared what the budget and production value for this going honestly sha\n",
      "not planning out just saying miss kissing you let out scoff shaking his head\n",
      "cnn tries conduct online trump hater poll but turns out landslide support for president\n",
      "focused niggas out here gettin extorted whales lmfaoooooooooooooooo\n",
      "shaking\n",
      "free willy out here gettin his bet they all got the rip shamoo tats too\n",
      "gonna california days shaking boots\n",
      "research examine aftermath earthquake caused blackout the press #earthquake\n",
      "reservoir water weight blamed for arkansas earthquake swarm paradise post #earthquake\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 15:01:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 38\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 15:01:00\n",
      "-------------------------------------------\n",
      "elf this woman has like the best job the world how does she it? would shaki\n",
      "remember blair hands shaking put his glasses read document\n",
      "landslide for trump weakest popular vote weakest\n",
      "nbin such cute ship also the spiciest ship the best both worlds hannah montana shaking her grave rip\n",
      "was that earthquake did you just rock world\n",
      "they sick our shit that for their homies stuck sea world\n",
      "this helps give people better heads that tropical system may making landfall the near future\n",
      "panther whales out here hittin licks seen all\n",
      "landslide footage from roundu valley gilgit baltistan via\n",
      "the modeling industry shaking are actual models\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 15:02:00\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 54\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-06-20 15:02:00\n",
      "-------------------------------------------\n",
      "cosmetology licenses are shaking\n",
      "tearway lmao keep shaking your head\n",
      "qatar accuses uae supporting attacks new york gulf crisis deepens\n",
      "national outreach direct usman ahmad speaking about #grenfelltower relief efforts\n",
      "they are shaking their heads the lying msm and fools like you that believe the lying msm\n",
      "snorlax available until lick earthquake\n",
      "alvin you drunk\n",
      "boats better have fish\n",
      "panther whales out here hittin licks seen all\n",
      "gangs aggressive killer whales are shaking down alaska fishing boats for their fish report\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination(timeout=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the author count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_counts_sorted_dstream = author_counts.transform(\\\n",
    "  (lambda foo:foo\\\n",
    "   .sortBy(lambda x:( -x[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "author_counts_sorted_dstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get top 5 authors by tweet count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_five_authors = author_counts_sorted_dstream.transform\\\n",
    "  (lambda rdd:sc.parallelize(rdd.take(5)))\n",
    "top_five_authors.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get authors with more than one tweet, or whose username starts with 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_authors = author_counts.filter(lambda x:\\\n",
    "                                                x[1]>1 \\\n",
    "                                                or \\\n",
    "                                                x[0].lower().startswith('rm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_authors.transform\\\n",
    "  (lambda rdd:rdd\\\n",
    "  .sortBy(lambda x:-x[1]))\\\n",
    "  .pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the most common words in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed.\\\n",
    "    flatMap(lambda tweet:tweet['text'].split(\" \"))\\\n",
    "    .countByValue()\\\n",
    "    .transform\\\n",
    "      (lambda rdd:rdd.sortBy(lambda x:-x[1]))\\\n",
    "    .pprint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
