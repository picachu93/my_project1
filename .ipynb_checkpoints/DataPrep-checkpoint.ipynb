{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "# CrisisLex\n",
    "# Author: Alexandra Olteanu\n",
    "# Check LICENSE for details about copyright.\n",
    "\n",
    "import json\n",
    "\n",
    "#receives a string in json format\n",
    "#returns the textual content of a tweet\n",
    "def extract_tweet_from_json(data):\n",
    "    try:\n",
    "        json_tweet = json.loads(data.strip())\n",
    "    except:\n",
    "        exit(\"Not able to load json data\")\n",
    "    if 'text' in json_tweet:\n",
    "        return json_tweet['text'].replace('\\n','')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# reads the terms to be tracked from a file\n",
    "# expects one term per line\n",
    "def get_query_terms(input_filename):\n",
    "    query_terms = []\n",
    "    for line in input_filename:\n",
    "        query_terms.append(line.strip())\n",
    "    return query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load config.py\n",
    "#Sets Twitter API access\n",
    "\n",
    "#Please give the keys as 'strings'\n",
    "CONSUMER_KEY = 'm4pfoQYtQOCahpKEY55dlogg7' # API key\n",
    "CONSUMER_SECRET = 'TdU3rKhwfspQYrSA0GsPW7IooRkbw9opfZ82KZmADsGvnAUL01' # API secret\n",
    "ACCESS_KEY = '798171014693879808-9I6Ms2tOWxLmXlwVbNHmEoYQJxO97rB' # Access token\n",
    "ACCESS_SECRET= '3vEM0E6i6OfmNE95ng8VcK6dHtKS6qByQoD7mZtp7qqvb' # Access token secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import tweepy1 as t\n",
    "from tweepy1.parsers import ModelParser\n",
    "from tweepy1 import StreamListener\n",
    "from tweepy1 import Stream\n",
    "\n",
    "\n",
    "import utils\n",
    "import config as c\n",
    "\n",
    "class PrintListener(StreamListener):\n",
    "    output = None\n",
    "    def on_data(self, data):\n",
    "        if self.output is None:\n",
    "            print data\n",
    "            return True\n",
    "        else:\n",
    "            print>>self.output, data.strip()\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            print status, \"Twitter API Error: Enhance your calm -- You are being rate limited\"\n",
    "        elif status == 401:\n",
    "            print status, \"Twitter API Error: Unauthorized -- Authentication credentials were missing or incorrect. Please double check config.py\"\n",
    "        else:\n",
    "            print status\n",
    "        \n",
    "    def set_output(self, output_json):\n",
    "        self.output = output_json\n",
    "\n",
    "#authenticate\n",
    "auth = t.OAuthHandler(c.CONSUMER_KEY, c.CONSUMER_SECRET)\n",
    "auth.set_access_token(c.ACCESS_KEY, c.ACCESS_SECRET)\n",
    "api = t.API(auth_handler=auth, parser = ModelParser())      \n",
    "        \n",
    "print \"Configuring query settings ....\"  \n",
    "    \n",
    "filename = 'data/test3.txt'\n",
    "lexicon = 'CrisisLexRec.txt'\n",
    "pl = PrintListener()\n",
    "pl.set_output(open(filename,\"w\"))\n",
    "\n",
    "try:\n",
    "        to_track = utils.get_query_terms(open(lexicon,\"r\"))\n",
    "except Exception as e:\n",
    "        print \"The file path is seems to be wrong. Check the error below or run the script with -h. Please revise and restart the script\"\n",
    "        print e\n",
    "        exit(0)\n",
    "        \n",
    "#start tracking crisis-relevant tweets\n",
    "stream = Stream(auth, pl)\n",
    "try:\n",
    "    print \"Collecting tweets ....\"\n",
    "    stream.filter(track=to_track[0:400],locations=[-6.38,49.87,1.77,55.81],languages=[\"en\"])\n",
    "except Exception as e:\n",
    "    print \"The script have crashed with the following error: \"\n",
    "    print e\n",
    "    print \"\\n Please check if your Twitter API keys are correct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load adaptive_collect.py\n",
    "# CrisisLex\n",
    "# Author: Alexandra Olteanu\n",
    "# Check LICENSE for details about copyright.\n",
    "\n",
    "import sys\n",
    "import utils\n",
    "import os\n",
    "\n",
    "#handling time\n",
    "import datetime\n",
    "\n",
    "#tweepy1\n",
    "import tweepy1 as t\n",
    "from tweepy1.parsers import ModelParser\n",
    "from tweepy1 import StreamListener\n",
    "from tweepy1 import Stream\n",
    "\n",
    "\n",
    "# authentication parameters\n",
    "import config as c\n",
    "\n",
    "#nlp processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#processes the tweet and updates hashtag_fd\n",
    "#specifically, if the hashtag was already encountered it adds it to the freq dict,\n",
    "# otherwise it increases the hashtag counter\n",
    "def update_hashtags_stats(hashtags_fd, json_tweet):\n",
    "    tweet = utils.extract_tweet_from_json(json_tweet)\n",
    "    tweet_terms = []\n",
    "    if tweet is None or '#' not in tweet:\n",
    "        return False\n",
    "    tokenizer = nltk.RegexpTokenizer('\\#?[\\w\\d]+')\n",
    "    doc = tokenizer.tokenize(tweet)\n",
    "    for w_raw in doc:\n",
    "        if '#' not in w_raw:\n",
    "            continue\n",
    "        w = (w_raw.strip('\\\"\\'.,;?!:)(@/*&')).lower()\n",
    "        tweet_terms.append(w)\n",
    "        hashtags_fd.inc(w)\n",
    "    return True\n",
    "\n",
    "\n",
    "#processes the tweet and updates terms_fd based on the tweet terms\n",
    "#specifically, if the term was already encountered it adds it to the freq dict,\n",
    "# otherwise it increases the term counter\n",
    "def update_terms_stats(terms_fd, json_tweet, lex):\n",
    "    tweet = utils.extract_tweet_from_json(json_tweet)\n",
    "    tweet_terms = []\n",
    "    if tweet is None:\n",
    "        return False\n",
    "    tokenizer = nltk.RegexpTokenizer('\\#?[\\w\\d]+')\n",
    "    doc = tokenizer.tokenize(tweet)\n",
    "    for w_raw in doc:\n",
    "        w = w_raw.strip('\\\"\\'.,;?!:)(@/*&')\n",
    "        if not (w.strip('#')).isalpha():\n",
    "            w_aux = ''\n",
    "            #ignore non-ascii characters\n",
    "            for s in w:\n",
    "                if ord(s) < 128:\n",
    "                    w_aux += s\n",
    "                else:\n",
    "                    break\n",
    "            w = w_aux\n",
    "        w = w.lower()\n",
    "        if (w not in stopwords.words('english') and w not in set(['rt','http','amp'])) and len(w) in range(3, 16):\n",
    "            if w in lex:\n",
    "                continue\n",
    "            tweet_terms.append(w)\n",
    "            terms_fd.inc(w)\n",
    "    bigrams = nltk.bigrams(tweet_terms)\n",
    "    for b in bigrams:\n",
    "        if b[1]+\" \"+b[0] in lex or b[0]+\" \"+b[1] in lex:\n",
    "            continue\n",
    "        if b[1]+\" \"+b[0] in terms_fd:\n",
    "            terms_fd.inc(b[1]+\" \"+b[0])\n",
    "        else:\n",
    "            terms_fd.inc(b[0]+\" \"+b[1])\n",
    "    return True\n",
    "\n",
    "\n",
    "class AdaptiveListener(StreamListener):\n",
    "    output = None\n",
    "    adaptive = False\n",
    "    start_time = None\n",
    "    end_time = None\n",
    "    terms = None\n",
    "    terms_fd = None\n",
    "    terms_no = 0\n",
    "    lex_set = None\n",
    "    use_hashtags = True\n",
    "\n",
    "    def on_data(self, data):\n",
    "        #prints to screen the filtered tweets\n",
    "        if self.output is None:\n",
    "            print data\n",
    "            return True\n",
    "\n",
    "        #prints to file the filtered tweets without any other actions\n",
    "        print>>self.output, data.strip()\n",
    "        if self.adaptive is False:\n",
    "            return True\n",
    "\n",
    "        #collects statistics from the tweets collected in the first hours and prints the tweets to file\n",
    "        if self.adaptive:\n",
    "            if datetime.datetime.now()>self.end_time:\n",
    "                self.terms = self.terms_fd.keys()[:self.terms_no]\n",
    "                print \"Adding to the query the following terms:\"\n",
    "                print self.terms\n",
    "                return False\n",
    "\n",
    "            if self.use_hashtags:\n",
    "                update_hashtags_stats(self.terms_fd,data)\n",
    "            else:\n",
    "                update_terms_stats(self.terms_fd,data,self.lex_set)\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            print status, \"Twitter API Error: Enhance your calm -- You are being rate limited\"\n",
    "        elif status == 401:\n",
    "            print status, \"Twitter API Error: Unauthorized -- Authentication credentials were missing or incorrect. Please double check config.py\"\n",
    "        else:\n",
    "            print status\n",
    "\n",
    "    def set_output(self, output_json):\n",
    "        self.output = output_json\n",
    "\n",
    "    def set_adaptive(self, lex, learning_time=3, use_hashtags=True, new_terms_no=10):\n",
    "        self.adaptive = True\n",
    "        self.lex_set = set(lex)\n",
    "        self.terms_no = new_terms_no\n",
    "        self.use_hashtags = use_hashtags\n",
    "\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.end_time = self.start_time + datetime.timedelta(hours=learning_time)\n",
    "        print \"Learning interval between %s to %s\"%(self.start_time,self.end_time)\n",
    "\n",
    "        self.terms_fd = nltk.FreqDist()\n",
    "        self.terms = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    filename = 'data/test3.txt'\n",
    "    lexicon = 'CrisisLexRec.txt'\n",
    "    prf_time = 1\n",
    "    adaptive = False\n",
    "    hashtags = 1\n",
    "    new_terms_no = 5\n",
    "\n",
    "    #authenticate\n",
    "    auth = t.OAuthHandler(c.CONSUMER_KEY, c.CONSUMER_SECRET)\n",
    "    auth.set_access_token(c.ACCESS_KEY, c.ACCESS_SECRET)\n",
    "    api = t.API(auth_handler=auth, parser=ModelParser())\n",
    "\n",
    "    #set up the stream listener\n",
    "    pl = AdaptiveListener()\n",
    "    pl.set_output(open(filename,\"w\"))\n",
    "\n",
    "    # get lexicon terms\n",
    "    try:\n",
    "        to_track = utils.get_query_terms(open(lexicon,\"r\"))\n",
    "    except Exception as e:\n",
    "        print \"The file path is seems to be wrong. Check the error below or run the script with -h. Please revise and restart the script\"\n",
    "        print e\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "    #set the learning time\n",
    "    if adaptive:\n",
    "        pl.set_adaptive(to_track, prf_time, (hashtags == 1), new_terms_no)\n",
    "\n",
    "    #start tracking crisis-relevant tweets\n",
    "    stream = Stream(auth, pl)\n",
    "    stream.filter(track=to_track[0:400],locations=[-6.38,49.87,1.77,55.81],languages=[\"en\"])\n",
    "\n",
    "    print \"New query...\"\n",
    "    #add new terms to be tracked and restart the crawling.\n",
    "    if pl.adaptive:\n",
    "        assert pl.terms\n",
    "        to_track[0:0] = pl.terms\n",
    "\n",
    "    pl.adaptive = False\n",
    "    stream.filter(track=to_track[0:400],locations=[-6.38,49.87,1.77,55.81],languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweets_data_path = './data/test3.txt'\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print len(tweets_data) \n",
    "\n",
    "tweets = pd.DataFrame()\n",
    "tweets['text'] = map(lambda tweet: tweet.get('text', None),tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet.get('lang', None),tweets_data)\n",
    "tweets['country'] = map(lambda tweet: tweet.get('place').get('country', None) if tweet.get('place') != None else None, tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Languages', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 languages', fontsize=15, fontweight='bold')\n",
    "tweets_by_lang[:5].plot(ax=ax, kind='bar', color='red')\n",
    "\n",
    "tweets_by_country = tweets['country'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=10)\n",
    "ax.set_xlabel('Countries', fontsize=15)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=15)\n",
    "ax.set_title('Top 5 countries', fontsize=15, fontweight='bold')\n",
    "tweets_by_country[:5].plot(ax=ax, kind='bar', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('2012_Sandy_Hurricane-ontopic_offtopic.csv','rU') as fin, open ('outfile2','w') as fout:\n",
    "    writer = csv.writer(fout, delimiter=',')\n",
    "    for row in csv.reader(fin, delimiter=','):\n",
    "        if row[2] == 'on-topic':\n",
    "             writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"outfile2\", \"rb\") as source:\n",
    "    lines = [line for line in source]\n",
    "import random\n",
    "random_choice = random.sample(lines, 3870)\n",
    "with open(\"outfile3\", \"wb\") as sink:\n",
    "    sink.write(\"\".join(random_choice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('nepal_pos.json') as tweet_data:\n",
    "    json_data = json.load(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import string\n",
    "\n",
    "tweets_data_path = 'nepal_neg.json'\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "fout = open('nepal_neg_txt.csv','w+')\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        date = tweet['created_at']\n",
    "        id = tweet['id']\n",
    "        if 'retweeted_status' in tweet:\n",
    "            text = tweet['retweeted_status']['text']\n",
    "        else:\n",
    "            text = tweet['text']\n",
    "        #print(text)    \n",
    "        try:\n",
    "            c = time.strptime(date.replace(\"+0000\",''), '%a %b %d %H:%M:%S %Y')\n",
    "        except: \n",
    "            print \"pb with tweet_gmttime\", tweet_gmttime, line\n",
    "            pass\n",
    "        tweet_unixtime = int(time.mktime(c))\n",
    "        #fout.write(str(text) + \"\\t\" + str(id) + \"\\n\")\n",
    "        fout.write(str([text]) + \"\\n\")\n",
    "        #print(tweet_unixtime)\n",
    "        #tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "tweets_file.close()        \n",
    "fout.close()         \n",
    "#tweets = pd.DataFrame()\n",
    "#tweets['text'] = map(lambda tweet: tweet.get('text', None),tweets_data)\n",
    "#tweets_by_text = tweets['text'].value_counts()\n",
    "#f1=open('output5.txt', 'w+')\n",
    "#print >> f1,tweets_by_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
