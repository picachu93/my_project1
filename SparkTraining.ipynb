{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "#Load a csv file and convert to DataFrame\n",
    "df = spark.read.option(\"header\",\"true\").csv(\"data/nepal_train.csv\")\n",
    "df = df.select(df.tweet_text,df.label.cast(\"double\").alias(\"label\"))\n",
    "df = df.dropna()\n",
    "#df.write.csv(\"data/mycsv.csv\")\n",
    "df.createOrReplaceTempView(\"tweets\")\n",
    "all_tweets = spark.sql(\"SELECT tweet_text, label FROM tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"tweet_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "tokenized = tokenizer.transform(all_tweets)\n",
    "#tok = tokenized.select(\"label\", \"words\")\n",
    "\n",
    "tok = tokenized.select(\"words\",\"label\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\")))\n",
    "#tok = tokenized.select(\"words\",\"tokens\",\"label\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+\n",
      "|          tweet_text|label|               words|\n",
      "+--------------------+-----+--------------------+\n",
      "|in pictures man i...|  0.0|[in, pictures, ma...|\n",
      "|still visiting ne...|  1.0|[still, visiting,...|\n",
      "|sending love to n...|  1.0|[sending, love, t...|\n",
      "|devastating love ...|  0.0|[devastating, lov...|\n",
      "|for so many years...|  1.0|[for, so, many, y...|\n",
      "|god this is nepal...|  0.0|[god, this, is, n...|\n",
      "|prayers for #nepa...|  1.0|[prayers, for, #n...|\n",
      "|more than killed ...|  0.0|[more, than, kill...|\n",
      "|like said these c...|  0.0|[like, said, thes...|\n",
      "|earthquake of mag...|  0.0|[earthquake, of, ...|\n",
      "|please tired of r...|  1.0|[please, tired, o...|\n",
      "|visit this link t...|  0.0|[visit, this, lin...|\n",
      "|      nepal fighting|  0.0|   [nepal, fighting]|\n",
      "|may god give stre...|  1.0|[may, god, give, ...|\n",
      "|#israelinnepal nu...|  1.0|[#israelinnepal, ...|\n",
      "|nepal#kathmanduqu...|  1.0|[nepal#kathmanduq...|\n",
      "|#nepal #uae nnepa...|  0.0|[#nepal, #uae, nn...|\n",
      "|breaking at least...|  0.0|[breaking, at, le...|\n",
      "|check out this in...|  1.0|[check, out, this...|\n",
      "|indeed our heart ...|  1.0|[indeed, our, hea...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered = remover.transform(tok)\n",
    "filtered = filtered.select(\"filtered\",\"tokens\",\"label\")\n",
    "#filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=3, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "\n",
    "ngramDataFrame = ngram.transform(tok)\n",
    "ngramDataFrame = ngramDataFrame.select(\"ngrams\")\n",
    "#ngramDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=50000)\n",
    "featurizedData = hashingTF.transform(filtered)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "tfidfData = idfModel.transform(featurizedData)\n",
    "tfidfData.select(\"filtered\", \"tokens\", \"features\",\"label\")#.show()\n",
    "pairs = tfidfData.select(\"label\",\"features\").rdd\n",
    "data = pairs.map(lambda x: LabeledPoint(x[0], as_mllib(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.10869968466\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "# Build the model\n",
    "model = SVMWithSGD.train(train, iterations=100)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(train.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/pythonSVMWithSGDModel\")\n",
    "#sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lets create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "pipeline.save(\"target/tmp/pythonHashingTFModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
