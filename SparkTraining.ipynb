{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "#Load a csv file and convert to DataFrame\n",
    "df = spark.read.option(\"header\",\"true\").csv(\"data/nepal_train.csv\")\n",
    "df = df.select(df.tweet_text,df.label.cast(\"double\").alias(\"label\"))\n",
    "df = df.dropna()\n",
    "#df.write.csv(\"data/mycsv.csv\")\n",
    "df.createOrReplaceTempView(\"tweets\")\n",
    "all_tweets = spark.sql(\"SELECT tweet_text, label FROM tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "only_tweets = spark.sql(\"SELECT tweet_text FROM tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet_text\", outputCol=\"words\")\n",
    "#regexTokenizer = RegexTokenizer(inputCol=\"tweet_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "tokenized = tokenizer.transform(all_tweets)\n",
    "#tok = tokenized.select(\"label\", \"words\")\n",
    "\n",
    "#tok = tokenized.select(\"words\",\"label\")\\\n",
    "#    .withColumn(\"tokens\", countTokens(col(\"words\")))\n",
    "#tok = tokenized.select(\"words\",\"tokens\",\"label\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered = remover.transform(tokenized)\n",
    "#filtered = filtered.select(\"filtered\",\"tokens\",\"label\")\n",
    "#filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=3, inputCol=\"filtered\", outputCol=\"ngrams\")\n",
    "ngrams = ngram.transform(filtered)\n",
    "#ngramDataFrame = ngramDataFrame.select(\"ngrams\")\n",
    "#ngramDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"ngrams\", outputCol=\"rawFeatures\", numFeatures=50000)\n",
    "featurizedData = hashingTF.transform(ngrams)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "tfidfData = idfModel.transform(featurizedData)\n",
    "pairs = tfidfData.select(\"label\",\"features\").rdd\n",
    "data = pairs.map(lambda x: LabeledPoint(x[0], as_mllib(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, ngram, hashingTF, idf])\n",
    "pipeline.save(\"target/tmp/pythonHashingTF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipe = Pipeline.load(\"target/tmp/pythonHashingTF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeModel = pipe.fit(all_tweets)\n",
    "pipeModel.save(\"target/tmp/pythonHashingTFModel_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "pipeModel1 = PipelineModel.load(\"target/tmp/pythonHashingTFModel_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|in pictures man i...|[in, pictures, ma...|[pictures, man, l...|[pictures man luc...|(50000,[487,3313,...|(50000,[487,3313,...|\n",
      "|still visiting ne...|[still, visiting,...|[still, visiting,...|[still visiting n...|(50000,[10542,295...|(50000,[10542,295...|\n",
      "|sending love to n...|[sending, love, t...|[sending, love, n...|[sending love nepal]|(50000,[42077],[1...|(50000,[42077],[8...|\n",
      "|devastating love ...|[devastating, lov...|[devastating, lov...|[devastating love...|(50000,[5812,2183...|(50000,[5812,2183...|\n",
      "|for so many years...|[for, so, many, y...|[many, years, mis...|[many years missi...|(50000,[10010,114...|(50000,[10010,114...|\n",
      "|god this is nepal...|[god, this, is, n...|[god, nepal, eart...|[god nepal earthq...|(50000,[1079,3240...|(50000,[1079,3240...|\n",
      "|prayers for #nepa...|[prayers, for, #n...|[prayers, #nepal,...|[prayers #nepal #...|(50000,[9830],[1.0])|(50000,[9830],[8....|\n",
      "|more than killed ...|[more, than, kill...|[killed, powerful...|[killed powerful ...|(50000,[4011,9600...|(50000,[4011,9600...|\n",
      "|like said these c...|[like, said, thes...|[like, said, used...|[like said used, ...|(50000,[2981,1152...|(50000,[2981,1152...|\n",
      "|earthquake of mag...|[earthquake, of, ...|[earthquake, magn...|[earthquake magni...|(50000,[536,8079,...|(50000,[536,8079,...|\n",
      "|please tired of r...|[please, tired, o...|[please, tired, r...|[please tired rol...|(50000,[4405,1153...|(50000,[4405,1153...|\n",
      "|visit this link t...|[visit, this, lin...|[visit, link, see...|[visit link see, ...|(50000,[825,7962,...|(50000,[825,7962,...|\n",
      "|      nepal fighting|   [nepal, fighting]|   [nepal, fighting]|                  []|       (50000,[],[])|       (50000,[],[])|\n",
      "|may god give stre...|[may, god, give, ...|[may, god, give, ...|[may god give, go...|(50000,[2832,9512...|(50000,[2832,9512...|\n",
      "|#israelinnepal nu...|[#israelinnepal, ...|[#israelinnepal, ...|[#israelinnepal n...|(50000,[14961,183...|(50000,[14961,183...|\n",
      "|nepal#kathmanduqu...|[nepal#kathmanduq...|[nepal#kathmanduq...|[nepal#kathmanduq...|(50000,[10677,233...|(50000,[10677,233...|\n",
      "|#nepal #uae nnepa...|[#nepal, #uae, nn...|[#nepal, #uae, nn...|[#nepal #uae nnep...|(50000,[4686,7484...|(50000,[4686,7484...|\n",
      "|breaking at least...|[breaking, at, le...|[breaking, least,...|[breaking least p...|(50000,[2385,9696...|(50000,[2385,9696...|\n",
      "|check out this in...|[check, out, this...|[check, interesti...|[check interestin...|(50000,[7105],[1.0])|(50000,[7105],[6....|\n",
      "|indeed our heart ...|[indeed, our, hea...|[indeed, heart, p...|[indeed heart pra...|(50000,[7308,3020...|(50000,[7308,3020...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_df = pipeModel1.transform(only_tweets)\n",
    "tf_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "splits = data.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.0777221294751\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "\n",
    "# Build the model\n",
    "model = SVMWithSGD.train(train, iterations=100)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = train.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(train.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "# Save and load model\n",
    "#model.save(sc, \"target/tmp/pythonSVMWithSGDModel\")\n",
    "#sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(sc, \"target/tmp/pythonSVMWithSGDModel_new\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
