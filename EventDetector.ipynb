{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Spark\n",
    "from pyspark import SparkContext\n",
    "#    Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "#    Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "#    json parsing\n",
    "import json\n",
    "#    PyMongo\n",
    "import pymongo\n",
    "#    Time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'created_at_1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MONGODB_URI = \"mongodb://localhost:27017/query2\"\n",
    "tweets_collection = \"predictions\"\n",
    "\n",
    "client = pymongo.MongoClient(MONGODB_URI)\n",
    "parsed_dburi = pymongo.uri_parser.parse_uri(MONGODB_URI)\n",
    "#       Database\n",
    "db = client[parsed_dburi['database']]\n",
    "\n",
    "#       Collection\n",
    "tweets_col = db[tweets_collection]\n",
    "#       Index\n",
    "#tweets.create_index(\"tweet_text\", unique=True)\n",
    "tweets_col.create_index(\"created_at\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zkQuorum = \"localhost:2181\"\n",
    "topic = \"twitter-stream\"\n",
    "batch_interval = 1\n",
    "window_length = 15 * batch_interval\n",
    "frequency = 6 * batch_interval\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "tweets = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the inbound message as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = tweets.map(lambda v: json.loads(v[1]))\n",
    "#parsed = parsed.filter(lambda tweet: tweet['geo']['coordinates'] != None)\n",
    "#parsed.filter(lambda tweet: tweet['metadata']['place']['country'] == 'United States')\n",
    "parsed.count().map(lambda x:'Tweets in this batch: %s' % x).pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dstream = parsed.map(lambda tweet: tweet['text'])\n",
    "#text_dstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract time from each tweet (edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_dstream = parsed.map(lambda tweet: tweet['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter tweets by location (edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-125.0011, 24.9493, -66.9326, 49.5904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_out_unicode(tweet):\n",
    "  \n",
    "    try:\n",
    "        clean_tweet = str(tweet)\n",
    "    except UnicodeEncodeError:\n",
    "        pass\n",
    "    return clean_tweet\n",
    "\n",
    "def expand_around_chars(text, characters):\n",
    "    for char in characters:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    return text\n",
    "\n",
    "def strip_quotations_newline(text):\n",
    "    clean_tweet = ' '.join(text.split())\n",
    "    clean_tweet = clean_tweet.encode('utf-8')\n",
    "    clean_tweet = clean_tweet.replace('\",\\'','')\n",
    "    clean_tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'''(@[A-Za-z0-9]+)''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"([0-9]+)\", \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'[^\\x00-\\x7F]+','', clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "def split_text(text):\n",
    "    text = strip_quotations_newline(text)\n",
    "    text = expand_around_chars(text, '\\/\".,()&[]{}:;!-_\\'')\n",
    "    splitted_text = text.split(' ')\n",
    "    cleaned_text = [x for x in splitted_text if len(x) > 2]\n",
    "    text_lowercase = [x.lower() for x in cleaned_text]\n",
    "    return text_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mess = text_dstream.map(lambda text: ' '.join(split_text(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bdc2333e636b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mess' is not defined"
     ]
    }
   ],
   "source": [
    "mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:55:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:55:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:55:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:55:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:04\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 16\n",
      "\n",
      "+----------+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                                                                 |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|0         |help mexico after the terrible earthquake september donate #fuerzamexico                                                   |\n",
      "|0         |heracross struggle bug earthquake until                                                                                    |\n",
      "|0         |lastromancist earth quake south korea                                                                                      |\n",
      "|0         |heracross counter earthquake until                                                                                         |\n",
      "|0         |heracross counter earthquake until                                                                                         |\n",
      "|0         |frida the rescue dog became symbol hope following the mexican earthquake                                                   |\n",
      "|0         |yoga supper education the world from that you can safe you flood earth quake volcano blast amp many more natural phenomenon|\n",
      "|0         |updated mag north north west kaikoura suburban canterbury new zealand                                                      |\n",
      "|0         |heracross struggle bug earthquake until                                                                                    |\n",
      "|1         |see that again joe clair amp earthquake trifling ass brothers amp not living above your means bill burr                    |\n",
      "|0         |almost all the tiny tremors not felt cascadia today were clustered near monday deep quake yellow                           |\n",
      "|0         |prayers are continuing with everyone who has been affected the earthquake mexico #prayformexico                            |\n",
      "|1         |baby bali earthquake bumps bangli and buleleng coconuts #bangli #balitoday                                                 |\n",
      "|0         |heracross counter earthquake until                                                                                         |\n",
      "|0         |did you forget when the clintons amp clinton foundation said they were taking donations help ear                           |\n",
      "|1         |photos amp video these are some the animals rescued after mexicos earthquake                                               |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:18\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n",
      "+----------+----------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                      |\n",
      "+----------+----------------------------------------------------------------+\n",
      "|0         |#earthquake #sismo strikes salina cruz #mexico min ago more info|\n",
      "+----------+----------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:36\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n",
      "+----------+-----------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                             |\n",
      "+----------+-----------------------------------------------------------------------+\n",
      "|0         |strong #earthquake #sismo mb= #sanpedrodeatacama #chile depth more info|\n",
      "+----------+-----------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:45\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n",
      "+----------+----------------------------------+\n",
      "|prediction|tweet_text                        |\n",
      "+----------+----------------------------------+\n",
      "|0         |heracross counter earthquake until|\n",
      "+----------+----------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-10-04 13:56:52\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession, DataFrameWriter\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "words = mess\n",
    "# Convert RDDs of the words DStream to DataFrame and run SQL query\n",
    "def process(time, rdd):\n",
    "\n",
    "    # Get the singleton instance of SparkSession\n",
    "    if not(rdd.isEmpty() and rdd != None):\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda w: Row(tweet_text=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame.\n",
    "        wordsDataFrame.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = \\\n",
    "            spark.sql(\"select tweet_text from tweets\")\n",
    "        pipeModel = PipelineModel.load(\"target/tmp/pythonHashingTFModel_new\")\n",
    "        # Extract features\n",
    "        tfidfData = pipeModel.transform(wordsDataFrame)\n",
    "        #tfidfData.show()\n",
    "        doo = tfidfData.select(\"features\").rdd\n",
    "        doo_data = doo.map(lambda y: LabeledPoint(0, as_mllib(y[0])))\n",
    "        doo_df = spark.createDataFrame(doo_data)\n",
    "        # Trained model \n",
    "        sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel_new\")\n",
    "        doo_lab = doo_data.map(lambda p: (p.label, sameModel.predict(p.features)))\n",
    "        doo_label = spark.createDataFrame(doo_lab)\n",
    "        fin = doo_label.selectExpr(\"_1 as label\", \"_2 as prediction\")\n",
    "        preds = fin.select(\"prediction\")\n",
    "        text = wordCountsDataFrame.withColumn(\"id\", monotonically_increasing_id())\n",
    "        preds = preds.withColumn(\"id\", monotonically_increasing_id())\n",
    "        text_preds = preds.join(text, \"id\", \"outer\").drop(\"id\")\n",
    "        \n",
    "        ## Running not removing duplicates though\n",
    "        #text_preds.dropDuplicates(['tweet_text'])\n",
    "        \n",
    "        text_preds.write.format(\"com.stratio.datasource.mongodb\").mode('append').options(host='localhost:27017',database='query2', collection='predictions').save()\n",
    "        #tweets_col.update_many({}, { '$set' : { \"created_at\" : datetime.datetime.utcnow() } }, True, True)\n",
    "        \n",
    "        #n_0 = text_preds.filter(text_preds.prediction != 1).count()\n",
    "        text_preds.show(40, truncate=False)\n",
    "\n",
    "words.foreachRDD(process)\n",
    "ssc.start()\n",
    "ssc.awaitTermination(timeout=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfView = spark.read.format('com.stratio.datasource.mongodb').options(host='localhost:27017', database='query2', collection='predictions').load()\n",
    "dfView.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_col.count({\"prediction\" : 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_col.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
