{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    Spark\n",
    "from pyspark import SparkContext\n",
    "#    Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "#    Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "#    json parsing\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Streaming Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zkQuorum = \"localhost:2181\"\n",
    "topic = \"twitter-stream\"\n",
    "batch_interval = 1\n",
    "window_length = 15 * batch_interval\n",
    "frequency = 6 * batch_interval\n",
    "ssc = StreamingContext(sc, batch_interval)\n",
    "\n",
    "tweets = KafkaUtils.createStream(ssc, zkQuorum, \"spark-streaming-consumer\", {topic: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the inbound message as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed = tweets.map(lambda v: json.loads(v[1]))\n",
    "#parsed = parsed.filter(lambda tweet: tweet['geo']['coordinates'] != None)\n",
    "#parsed = parsed.filter(lambda tweet: tweet['metadata']['place']['country'] == 'United States')\n",
    "parsed.count().map(lambda x:'Tweets in this batch: %s' % x).pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_dstream = parsed.map(lambda tweet: tweet['text'])\n",
    "#text_dstream.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract time from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_dstream = parsed.map(lambda tweet: tweet['created_at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter tweets by location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-125.0011, 24.9493, -66.9326, 49.5904"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_out_unicode(tweet):\n",
    "  \n",
    "    try:\n",
    "        clean_tweet = str(tweet)\n",
    "    except UnicodeEncodeError:\n",
    "        pass\n",
    "    return clean_tweet\n",
    "\n",
    "def expand_around_chars(text, characters):\n",
    "    for char in characters:\n",
    "        text = text.replace(char, ' ' + char + ' ')\n",
    "    return text\n",
    "\n",
    "def strip_quotations_newline(text):\n",
    "    clean_tweet = ' '.join(text.split())\n",
    "    clean_tweet = clean_tweet.encode('utf-8')\n",
    "    clean_tweet = clean_tweet.replace('\",\\'','')\n",
    "    clean_tweet = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'''(@[A-Za-z0-9]+)''', \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(\"([0-9]+)\", \"\", clean_tweet)\n",
    "    clean_tweet = re.sub(r'[^\\x00-\\x7F]+','', clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "def split_text(text):\n",
    "    text = strip_quotations_newline(text)\n",
    "    text = expand_around_chars(text, '\\/\".,()&[]{}:;!-_\\'')\n",
    "    splitted_text = text.split(' ')\n",
    "    cleaned_text = [x for x in splitted_text if len(x) > 2]\n",
    "    text_lowercase = [x.lower() for x in cleaned_text]\n",
    "    return text_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mess = text_dstream.map(lambda text: ' '.join(split_text(text)))\n",
    "#mess.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load feature extraction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMModel\n",
    "#sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "#PipelineModel.load(\"target/tmp/pythonHashingTFModel_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:12\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 7\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|there reason they...|[there, reason, t...|[reason, call, ea...|[reason call eart...|(50000,[834,1431,...|(50000,[834,1431,...|\n",
      "|santa clara snorl...|[santa, clara, sn...|[santa, clara, sn...|[santa clara snor...|(50000,[216,8803,...|(50000,[216,8803,...|\n",
      "|#earthquake strik...|[#earthquake, str...|[#earthquake, str...|[#earthquake stri...|(50000,[18834,206...|(50000,[18834,206...|\n",
      "|beachy california...|[beachy, californ...|[beachy, californ...|[beachy californi...|(50000,[859,33403...|(50000,[859,33403...|\n",
      "|#wimbledon earthq...|[#wimbledon, eart...|[#wimbledon, eart...|[#wimbledon earth...|(50000,[312,6187,...|(50000,[312,6187,...|\n",
      "|earthquake occurr...|[earthquake, occu...|[earthquake, occu...|[earthquake occur...|(50000,[5400,2190...|(50000,[5400,2190...|\n",
      "|#naturaldisasters...|[#naturaldisaster...|[#naturaldisaster...|[#naturaldisaster...|(50000,[4593,3058...|(50000,[4593,3058...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+-----------------------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                                     |\n",
      "+----------+-----------------------------------------------------------------------------------------------+\n",
      "|0         |#earthquake strikes #srnagar #india min ago more info                                          |\n",
      "|0         |there reason they call this earthquake bar eat pizza bench heavy                               |\n",
      "|1         |#naturaldisastersnews bmkg japan conduct joint research related earthquake                     |\n",
      "|1         |beachy californian didn even feel the earthquake                                               |\n",
      "|0         |santa clara snorlax lick earthquake hatched available until                                    |\n",
      "|0         |#wimbledon earthquake rafael nadal beaten gilles mller after epic wimbledon really thriller was|\n",
      "|0         |earthquake occurred near uttaranchal india utc #earthquake #uttaranchal                        |\n",
      "+----------+-----------------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:17\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 2\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|fresh shallow mag...|[fresh, shallow, ...|[fresh, shallow, ...|[fresh shallow ma...|(50000,[625,6634,...|(50000,[625,6634,...|\n",
      "|earthquake flood ...|[earthquake, floo...|[earthquake, floo...|[earthquake flood...|(50000,[1657,1156...|(50000,[1657,1156...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+-----------------------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                                     |\n",
      "+----------+-----------------------------------------------------------------------------------------------+\n",
      "|0         |earthquake flood damage and hotness ization are continuing japan like refuge and returning home|\n",
      "|0         |fresh shallow magnitude earthquake rocks leyte philippines                                     |\n",
      "+----------+-----------------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:18\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:19\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|anyone remember w...|[anyone, remember...|[anyone, remember...|[anyone remember ...|(50000,[696,4565,...|(50000,[696,4565,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                                                                |\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "|1         |anyone remember when chelsea clinton used money meant for haiti earthquake relief for her wedding? like mommy like chelsea|\n",
      "+----------+--------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:32\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:33\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:35\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:36\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:37\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:38\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:48\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:49\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:50\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:51\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:52\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:53\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:54\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:55\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|unlikely bedfello...|[unlikely, bedfel...|[unlikely, bedfel...|[unlikely bedfell...|(50000,[9750,1068...|(50000,[9750,1068...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                                                     |\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+\n",
      "|1         |unlikely bedfellows rally oppose seismic air gun testing offshore drilling near the north carolina coast #ncpol|\n",
      "+----------+---------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:56\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:57\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:58\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:06:59\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:00\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:01\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:02\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:03\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:04\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:06\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:07\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:08\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:11\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 3\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|earthquake magnit...|[earthquake, magn...|[earthquake, magn...|[earthquake magni...|(50000,[3125,7737...|(50000,[3125,7737...|\n",
      "|and here more gam...|[and, here, more,...|[game, camera, fo...|[game camera foot...|(50000,[17128,243...|(50000,[17128,243...|\n",
      "|got rich because ...|[got, rich, becau...|[got, rich, faile...|[got rich failed,...|(50000,[11361,370...|(50000,[11361,370...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+---------------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                             |\n",
      "+----------+---------------------------------------------------------------------------------------+\n",
      "|0         |and here more game camera footage from minto flats seismic station lynx moose bears etc|\n",
      "|0         |earthquake magnitude occurred ist lat amp long depth region chamoli uttarakhand        |\n",
      "|0         |got rich because failed perdict earthquake                                             |\n",
      "+----------+---------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:14\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:16\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:18\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|magnitude quake h...|[magnitude, quake...|[magnitude, quake...|[magnitude quake ...|(50000,[906,3653,...|(50000,[906,3653,...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+-------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                     |\n",
      "+----------+-------------------------------------------------------------------------------+\n",
      "|1         |magnitude quake has been recorded near #foster #exclamations this morning woman|\n",
      "+----------+-------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:20\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:23\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:24\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:26\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:27\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:28\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:29\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:30\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:31\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:32\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 3\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|just earthquake w...|[just, earthquake...|[earthquake, prel...|[earthquake preli...|(50000,[2114,3138...|(50000,[2114,3138...|\n",
      "|conservation acti...|[conservation, ac...|[conservation, ac...|[conservation act...|(50000,[9172,2624...|(50000,[9172,2624...|\n",
      "|      eno earthquake|   [eno, earthquake]|   [eno, earthquake]|                  []|       (50000,[],[])|       (50000,[],[])|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+-------------------------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                                       |\n",
      "+----------+-------------------------------------------------------------------------------------------------+\n",
      "|1         |conservation action join the broad coalition opposing seismic testing via                        |\n",
      "|0         |just earthquake with preliminary magnitude rattles northern california between eureka and redding|\n",
      "|0         |eno earthquake                                                                                   |\n",
      "+----------+-------------------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:33\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|this fairly alarm...|[this, fairly, al...|[fairly, alarming...|[fairly alarming ...|(50000,[4848,2492...|(50000,[4848,2492...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+----------+--------------------------------------------------------------------------------------------------+\n",
      "|prediction|tweet_text                                                                                        |\n",
      "+----------+--------------------------------------------------------------------------------------------------+\n",
      "|1         |this fairly alarming when you consider only miles from the gigantic supervolcano known yellowstone|\n",
      "+----------+--------------------------------------------------------------------------------------------------+\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:34\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-07-11 01:07:35\n",
      "-------------------------------------------\n",
      "Tweets in this batch: 1\n",
      "\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          tweet_text|               words|            filtered|              ngrams|         rawFeatures|            features|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|beachy california...|[beachy, californ...|[beachy, californ...|[beachy californi...|(50000,[859,33403...|(50000,[859,33403...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row, SparkSession, DataFrameWriter\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.linalg import Vector as MLVector, Vectors as MLVectors\n",
    "from pyspark.mllib.linalg import Vector as MLLibVector, Vectors as MLLibVectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession\\\n",
    "            .builder\\\n",
    "            .config(conf=sparkConf)\\\n",
    "            .getOrCreate()\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "words = mess\n",
    "# Convert RDDs of the words DStream to DataFrame and run SQL query\n",
    "def process(time, rdd):\n",
    "\n",
    "    # Get the singleton instance of SparkSession\n",
    "    if not(rdd.isEmpty() and rdd != None):\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        rowRdd = rdd.map(lambda w: Row(tweet_text=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame.\n",
    "        wordsDataFrame.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = \\\n",
    "            spark.sql(\"select tweet_text from tweets\")\n",
    "        pipeModel = PipelineModel.load(\"target/tmp/pythonHashingTFModel_new\")\n",
    "        # Extract features\n",
    "        tfidfData = pipeModel.transform(wordsDataFrame)\n",
    "        tfidfData.show()\n",
    "        doo = tfidfData.select(\"features\").rdd\n",
    "        doo_data = doo.map(lambda y: LabeledPoint(0, as_mllib(y[0])))\n",
    "        doo_df = spark.createDataFrame(doo_data)\n",
    "        # Trained model \n",
    "        sameModel = SVMModel.load(sc, \"target/tmp/pythonSVMWithSGDModel_new\")\n",
    "        doo_lab = doo_data.map(lambda p: (p.label, sameModel.predict(p.features)))\n",
    "        doo_label = spark.createDataFrame(doo_lab)\n",
    "        fin = doo_label.selectExpr(\"_1 as label\", \"_2 as prediction\")\n",
    "        preds = fin.select(\"prediction\")\n",
    "        text = wordCountsDataFrame.withColumn(\"id\", monotonically_increasing_id())\n",
    "        preds = preds.withColumn(\"id\", monotonically_increasing_id())\n",
    "        text_preds = preds.join(text, \"id\", \"outer\").drop(\"id\")\n",
    "        #n_0 = text_preds.filter(text_preds.prediction != 1).count()\n",
    "        \n",
    "        #threshold = 3\n",
    "        #if num_occurs > threshold:\n",
    "        #text_preds.write.saveAsTable('test_table', format='parquet', mode='append',path='data/test_tab')\n",
    "        \n",
    "        text_preds.show(40, truncate=False)\n",
    "\n",
    "words.foreachRDD(process)\n",
    "ssc.start()\n",
    "ssc.awaitTermination(timeout=180)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
